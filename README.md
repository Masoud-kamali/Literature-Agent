# Literature Agent for 3D Gaussian Splatting

An autonomous research agent that discovers, analyzes, and generates publication-ready summaries of new research papers on 3D Gaussian Splatting using locally-hosted LLMs via vLLM.

## Features

- **Multi-source Retrieval**: Fetches papers from arXiv, OpenAlex, and CVF Open Access (CVPR/ICCV/ECCV)
- **Reddit Integration**: Monitors r/PlayCanvas and r/GaussianSplatting for tools and community discussions
- **Flexible Output Formats**: Generate posts with 3 papers OR 2 papers + 1 tool/post
- **Local LLM Integration**: Uses vLLM with OpenAI-compatible API (Llama-3.1-8B-Instruct)
- **Two-Agent Reflection System**: 
  - **Critic Agent**: Evaluates for conciseness, engagement, and precision
  - **Reviser Agent**: Shortens text, adds engaging verbs, ensures smooth transitions
- **Smart Descriptions**: 3-sentence flowing paragraphs with smooth transitions and precise metrics
- **Smart Deduplication**: CSV ledger with unlimited retry until target papers found
- **Australian English**: All outputs in Australian English with professional tone
- **Combined LinkedIn Posts**: Generates publication-ready posts with attractive descriptions
- **Production Ready**: Robust error handling, retry logic, and rate limiting

## Generated Outputs

The agent generates a combined LinkedIn post titled "3D Gaussian Splatting ‚Äî Weekly Literature Update" featuring:

**Default Format (2 papers + 1 tool):**
- **2 research papers** from arXiv, OpenAlex, and CVF conferences
- **1 relevant tool/discussion** from Reddit (r/PlayCanvas, r/GaussianSplatting)

**Alternative Format (3 papers):**
- **3 research papers** from academic sources only

For each item:
1. **Concise Description** (3 flowing sentences with smooth transitions)
   - Sentence 1: Innovation/problem addressed
   - Sentence 2: Method/technique used (with transition)
   - Sentence 3: Precise results with metrics
2. **Metadata**: Title, authors/source, and URL

### Quality Control Pipeline

All outputs undergo **two-stage processing**:

1. **Draft Generation**: Initial LLM output with strict prompts
2. **Reflection Agent** (LangGraph-based):
   - **Critic Node**: Reviews for factuality, conciseness, interactivity, and precision
   - **Reviser Node**: Shortens text, adds engaging verbs, ensures smooth transitions, and includes specific metrics

The result: **Smooth, engaging, LinkedIn-ready descriptions** with natural flow and precise results.

See [REDDIT_FEATURE.md](REDDIT_FEATURE.md) for details on the Reddit integration.

## Architecture

```
literature-agent/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Pydantic settings
‚îÇ   ‚îú‚îÄ‚îÄ clients/               # Data source clients
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ arxiv_client.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openalex_client.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cvf_client.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reddit_client.py   # Reddit integration
‚îÇ   ‚îú‚îÄ‚îÄ dedupe/                # Deduplication logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ledger.py          # CSV ledger management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ normalise.py       # Title normalisation + hashing
‚îÇ   ‚îú‚îÄ‚îÄ llm/                   # LLM integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vllm_chat.py       # OpenAI-compatible vLLM client
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.py         # All prompt templates
‚îÇ   ‚îú‚îÄ‚îÄ agents/                # Agent orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py        # Main orchestration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reflection.py      # LangGraph reflection agent
‚îÇ   ‚îú‚îÄ‚îÄ output/                # Output writers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ writer.py
‚îÇ   ‚îî‚îÄ‚îÄ publish/               # Publishing stubs
‚îÇ       ‚îî‚îÄ‚îÄ linkedin.py
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ run_weekly.py          # Main CLI runner
‚îÇ   ‚îî‚îÄ‚îÄ backfill.py            # Historical backfill
‚îú‚îÄ‚îÄ tests/                     # Unit tests
‚îú‚îÄ‚îÄ data/                      # Ledger storage
‚îú‚îÄ‚îÄ output/                    # Generated reports
‚îî‚îÄ‚îÄ logs/                      # Application logs
```

## Installation

### Prerequisites

- Python 3.10+
- CUDA-capable GPU (for vLLM)
- Poetry (package manager)

### Setup Steps

1. **Clone and Install Dependencies**

```bash
git clone https://github.com/Masoud-kamali/Literature-Agent.git
cd Literature-Agent
poetry install
```

2. **Configure Environment Variables** (optional)

Create a `.env` file:

```bash
# vLLM settings
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
VLLM_TEMPERATURE=0.7
VLLM_MAX_TOKENS=1024

# OpenAlex polite pool (recommended)
OPENALEX_MAILTO=your.email@example.edu.au

# LinkedIn (dry-run by default)
LINKEDIN_DRY_RUN=true

# Reddit settings
REDDIT_SUBREDDITS=["PlayCanvas", "GaussianSplatting"]  
OUTPUT_FORMAT=2_papers_1_tool  # or "3_papers" for papers only

# LLM settings (lower temperature for consistency)
VLLM_TEMPERATURE=0.3
REFLECTION_TEMPERATURE=0.3

# Logging
LOG_LEVEL=INFO
```

3. **Start vLLM Server**

The agent requires a running vLLM server. Use the provided startup script:

```bash
# Create conda/mamba environment for vLLM
mamba create -n vllm python=3.10 -y
mamba activate vllm
pip install vllm

# Start vLLM server (script included)
bash scripts/setup/start_vllm.sh
```

Or manually:

```bash
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-3.1-8B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype auto \
  --max-model-len 4096 \
  --gpu-memory-utilization 0.75 \
  --disable-custom-all-reduce
```

**Note**: The first run will download the model weights (~16GB for 8B model). You must have access to the Llama model on HuggingFace.

4. **Authenticate with HuggingFace** (required for Llama models)

```bash
pip install huggingface_hub
huggingface-cli login
# Enter your HuggingFace token when prompted
```

5. **Verify vLLM is Running**

```bash
curl http://localhost:8000/v1/models
```

You should see JSON output with your model name.

## Usage

### Basic Run

Process papers from the last 7 days:

```bash
poetry run python scripts/run_weekly.py
```

The agent will:
1. Fetch papers from arXiv, OpenAlex, and CVF conferences
2. Deduplicate against the ledger
3. Keep fetching with increasing batch sizes until 3 new papers are found
4. Generate technical abstracts and problem statements
5. Apply reflection (critique and revision)
6. Create a combined LinkedIn post with all 3 papers
7. Save outputs to `output/week_YYYY_MM_DD/`

### Custom Lookback Period

Process papers from the last 14 days:

```bash
poetry run python scripts/run_weekly.py --days 14
```

**Note**: CVF conferences are filtered by date range. For example, with `--days 30`, only years from the last 30 days are checked (e.g., if running in January 2026, only 2026 and 2025 conferences are searched).

### Increase Initial Batch Size

Start with larger batch (default is from config):

```bash
poetry run python scripts/run_weekly.py --max_results 100
```

### Debug Mode

Run with detailed logging:

```bash
poetry run python scripts/run_weekly.py --log_level DEBUG
```

## Outputs

After each run, outputs are saved to `output/week_YYYY_MM_DD/`:

- **`weekly_report.md`**: Combined markdown report with:
  - Individual sections for each paper (abstract rewrite, problem statement, individual LinkedIn post)
  - **Combined LinkedIn Post** section with all 3 papers in publication-ready format
- **Individual JSON files**: One per paper with all metadata and generated content

Example Combined LinkedIn Post Format:

```
üîç What's New in 3D Gaussian Splatting: Recent Tools & Research

üõ†Ô∏è Synthetic 3DGS from Games is totally underrated
üë§ u/Dung3onlord ‚Ä¢ r/GaussianSplatting
üîó https://reddit.com/...

Community guide for generating 3DGS from game engines. Covers camera 
arrays, AI tools, and synthetic video methods. Provides practical insights 
for developers and researchers.

üìÑ OceanSplat: Object-aware Gaussian Splatting
üë• Minseong Kweon; Jinsun Park
üîó http://arxiv.org/pdf/...

OceanSplat tackles underwater 3D reconstruction with trinocular consistency. 
The approach enforces geometric constraints via inverse warping and depth 
regularization. Experiments demonstrate 30% fewer artifacts and superior 
reconstruction quality compared to baseline methods.

üìÑ ProFuse: Efficient Cross-View Context Fusion
üë• Yen-Jen Chiou; Wei-Tse Cheng; Yuan-Fu Yang
üîó http://arxiv.org/pdf/...

ProFuse proposes efficient context fusion for open-vocabulary 3D scenes. 
The approach employs dense correspondence-guided pre-registration with 
cross-view clustering. Experiments demonstrate 2x faster semantic attachment 
and state-of-the-art accuracy on benchmark datasets.

#ComputerVision #3DGaussianSplatting #GenAI #Research
```

Example JSON output:

```json
{
  "canonical_id": "2401.12345",
  "source": "arxiv",
  "title": "3D Gaussian Splatting for Real-Time Rendering",
  "authors": "Smith, A.; Johnson, B.",
  "year": 2024,
  "url": "https://arxiv.org/pdf/2401.12345.pdf",
  "abstract_rewrite": "This paper introduces...",
  "problem_solved": "Traditional rendering methods...",
  "linkedin_post": "New Research Alert...",
  "model_name": "meta-llama/Llama-3.1-8B-Instruct"
}
```

## Deduplication

The agent maintains a CSV ledger at `data/ledger.csv` with all processed papers. Each paper is identified by:

1. **arXiv ID** (if from arXiv)
2. **DOI** (if available from OpenAlex)
3. **Stable hash** of normalized title (for CVF papers)

On each run, the agent:
- Loads the ledger into memory
- Skips any paper already in the ledger
- **Keeps fetching with increasing batch sizes** until 3 new papers are found (no attempt limit)
- Appends new papers after successful processing
- Saves the ledger atomically

This ensures:
- No duplicates across runs
- Always generates exactly 3 new papers (unless source is exhausted)
- Efficient handling of high-duplication scenarios

## Rate Limiting and Polite Crawling

The agent respects rate limits for all data sources:

- **arXiv**: 3-second delay between requests (configurable)
- **OpenAlex**: 0.1-second delay + polite pool `mailto` parameter
- **CVF**: 2-second delay between page requests

All HTTP calls use exponential backoff retry logic (3 attempts by default).

## Reflection Agent

The reflection process (powered by LangGraph):

1. **Draft Generation**: Generate initial outputs using prompts
2. **Critic Node**: LLM reviews outputs for:
   - Factuality (no claims beyond abstract)
   - Specificity (technical details preserved)
   - Novelty framing (contribution clarity)
   - Style (Australian English, academic tone)
   - Length constraints
3. **Decision**: If score ‚â• 8/10, accept; otherwise proceed to revision
4. **Reviser Node**: Apply critic's suggested actions to improve outputs
5. **Output**: Return final refined outputs

The critic uses lower temperature (0.3) for consistent evaluation.

## Testing

Run unit tests:

```bash
poetry run pytest tests/ -v
```

Run with coverage:

```bash
poetry run pytest tests/ --cov=src --cov-report=html
```

Tests cover:
- Title normalisation and hashing
- Ledger operations (add, save, load, dedupe)
- Mock end-to-end pipeline run

## LinkedIn Publishing

By default, the agent runs in **dry-run mode**: it generates LinkedIn posts but does not publish them.

To enable actual posting:

1. Create a LinkedIn App at https://www.linkedin.com/developers/apps
2. Configure OAuth 2.0 and obtain access token
3. Implement the OAuth flow in `src/publish/linkedin.py`
4. Set `LINKEDIN_DRY_RUN=false` in your `.env`

For detailed LinkedIn API integration, see the instructions printed by:

```bash
poetry run python scripts/run_weekly.py
```

## Configuration Reference

Key settings in `src/config.py` (overridable via environment variables):

| Variable | Default | Description |
|----------|---------|-------------|
| `VLLM_BASE_URL` | `http://localhost:8000/v1` | vLLM API endpoint |
| `VLLM_MODEL_NAME` | `meta-llama/Llama-3.1-8B-Instruct` | Model name |
| `VLLM_TEMPERATURE` | `0.7` | Sampling temperature |
| `VLLM_MAX_TOKENS` | `1024` | Max tokens per generation |
| `OPENALEX_MAILTO` | `researcher@example.edu.au` | Polite pool email |
| `DEFAULT_DAYS_BACK` | `7` | Default lookback period |
| `MAX_RESULTS_PER_SOURCE` | `50` | Max papers per source |
| `REFLECTION_MAX_ITERATIONS` | `1` | Reflection cycles |
| `LINKEDIN_DRY_RUN` | `true` | Dry-run mode for posts |

## Prompts

All prompts are defined in `src/llm/prompts.py`:

- **SYSTEM_PROMPT**: Base system message (academic writer persona)
- **ABSTRACT_REWRITE_PROMPT**: Technical abstract rewrite
- **PROBLEM_STATEMENT_PROMPT**: Problem being solved
- **LINKEDIN_POST_PROMPT**: LinkedIn-ready post
- **CRITIC_PROMPT**: Reflection critic evaluation
- **REVISER_PROMPT**: Reflection revision application

These prompts enforce Australian English spelling and academic tone.

## Troubleshooting

### vLLM Connection Errors

**Problem**: `Connection refused` or timeout errors

**Solutions**:
- Verify vLLM server is running: `curl http://localhost:8000/v1/models`
- Check firewall rules if running on remote server
- Ensure correct `VLLM_BASE_URL` in config

### Out of Memory (OOM) Errors

**Problem**: vLLM crashes with CUDA OOM

**Solutions**:
- Use smaller model (e.g., 8B instead of 70B)
- Reduce `--max-model-len` in vLLM startup
- Lower `--gpu-memory-utilization` (default 0.9)
- Enable tensor parallelism for multi-GPU

### No Papers Retrieved

**Problem**: Agent reports 0 papers found

**Solutions**:
- Verify internet connectivity
- Check if date range is too narrow (increase `--days`)
- Verify search keywords in `config.py` are relevant
- Check API status for arXiv/OpenAlex

### JSON Parsing Errors in Reflection

**Problem**: Reflection agent fails to parse LLM output

**Solutions**:
- This is expected occasionally with open models
- The agent has fallback logic (accepts draft outputs)
- Try a more capable model (e.g., 70B or Llama-3.3-70B-Instruct)
- Adjust prompts in `src/llm/prompts.py` for your model

## Example Run Output

```
2024-01-15 09:00:00 | INFO     | Starting Literature Agent Pipeline
2024-01-15 09:00:05 | INFO     | Retrieved 45 papers from arXiv
2024-01-15 09:00:08 | INFO     | Retrieved 23 papers from OpenAlex
2024-01-15 09:00:12 | INFO     | Retrieved 8 papers from CVF
2024-01-15 09:00:12 | INFO     | Filtered to 12 new papers (skipped 64 duplicates)
2024-01-15 09:00:45 | INFO     | Generating outputs for: 3D Gaussian Splatting for Real-Time Rendering
2024-01-15 09:01:20 | INFO     | Reflection: Running critic node
2024-01-15 09:01:35 | INFO     | Critique score: 9/10, 0 actions
2024-01-15 09:01:35 | INFO     | Accepting outputs (score=9, iteration=0)
2024-01-15 09:01:35 | INFO     | Completed processing: 3D Gaussian Splatting for Real-Time Rendering
...
2024-01-15 09:15:20 | INFO     | Pipeline complete: 12 papers processed
2024-01-15 09:15:21 | INFO     | Outputs written to: output/week_2024_01_15
```

## Dependencies

Core dependencies (managed by Poetry):

- **httpx**: Async HTTP client
- **beautifulsoup4 + lxml**: HTML parsing for CVF
- **feedparser**: arXiv API feed parsing
- **openai**: OpenAI-compatible client for vLLM
- **langgraph**: Reflection agent state machine
- **pydantic**: Configuration and validation
- **loguru**: Structured logging
- **tenacity**: Retry logic
- **pandas**: Ledger CSV handling

## Contributing

To extend the agent:

1. **Add new data sources**: Implement client in `src/clients/`
2. **Modify prompts**: Edit templates in `src/llm/prompts.py`
3. **Customise reflection**: Adjust graph in `src/agents/reflection.py`
4. **Add output formats**: Extend `src/output/writer.py`

## License

Apache License 2.0 - see [LICENSE](LICENSE) file.

## Citation

If you use this agent in your research workflow, please cite:

```bibtex
@software{literature_agent_2026,
  title={Literature Agent for 3D Gaussian Splatting},
  author={Kamali, Masoud},
  year={2026},
  url={https://github.com/Masoud-kamali/Literature-Agent}
}
```

## Acknowledgements

- vLLM team for efficient local inference
- OpenAlex for open academic data
- arXiv for open access preprints
- CVF for conference open access

---

**Questions or Issues?** Please open an issue on GitHub or contact the maintainers.
